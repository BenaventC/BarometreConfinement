---
title: "Vectorisation"
author: "CB"
date: "1 avril 2020"
output: html_document
---

```{r setup,include=FALSE}
knitr::opts_chunk$set(include=TRUE,echo = TRUE, warning=FALSE,  message=FALSE, cache=TRUE,fig.width = 8)
if (!require(wordVectors)) {
  if (!(require(devtools))) {
    install.packages("devtools")
  }
  devtools::install_github("bmschmidt/wordVectors")
}
library(readr)
library(stringi)
library(tidyverse)
library(gridExtra)
library(cleanNLP) 
library(text2vec)
library(reshape2)
library(wordVectors)
library(tsne)
library(ape)
```

## L'objectif de la vectorisation :

Un outil pour constituer des dictionnaires spécifiques. Pour le corpus Confinementjour les thèmes de prédictions sont les marqueurs de l'expérience du confinement :

 - l'ennui
 - la peur
 - l'ironie
 - agrements et désagrément du teletravail
 - le rapports aux enfants
 - la déréalisation , science fiction
 - l'approvisionnement et l'alimentation 
 
 l'enjeu : retrouver les termes qui se rapproche le plus du concept que l'on cherche à cerner.
 
 Processus : 
 1) analyse quali des tweet et définition des thèmes
 2) établissement d'un petits nombre de mots cible, utilisation de la synonymie ( Wolf, https://wonef.fr/contact/)
 3) recherche des vecteurs les plus proches et 
 4) réitération en deux
 
 On travaille sur des lemmes.
 
```{r prep}
if (!file.exists("confinement_vec.txt")) {
  #si le fichier n'existe pas, on lit nos données brutes, et on nettoie le corpus des stop words
  cnlp_init_udpipe(model_name = "french")
  #lecture de l'ensemble de nos tweets
  obj<-readRDS(file = "df_sample.rds")
  #suppression des emojis, handlers, hashtags et URLs
  cleanText <- function(x) {
       gsub("([\U{1F300}-\U{1F9FF}])*(@\\S+)*(#\\S+)*(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)*", "", x)
  }
  obj['text'] = lapply(obj['text'], cleanText)
  #on enlève les tweets qui ne contenaient que des emojis, hashtags ou urls.
  obj = obj[grepl("\\w+",obj$text),]
  #Annotation des tweets afin de pouvoir identifier les stopwords
  Vocab<-cnlp_annotate(obj)
  #filtrage sur les stopwords
  updated_vocab = filter(Vocab$token, !(upos %in% c('ADP','DET','SCONJ', 'PRON', 'NUM')))
  all_tweets <- paste(updated_vocab['token'], sep= " ")
  write.table(all_tweets, file="tweets/tweets.txt")
  #Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
  prep_word2vec(origin="tweets",destination="confinement_vec.txt",lowercase=T,bundle_ngrams=3)
}
#Création et entraînement du modèle vectoriel
if (!file.exists("confinement_vec_model.bin")) {
  model = train_word2vec("confinement_vec.txt","confinement_vec_model.bin",vectors=100
                         ,threads=4,window=10,iter=5,negative_samples=0)
} else model = read.vectors("confinement_vec_model.bin")
```
```{r}
#Un premier clustering
set.seed(10)
centers = 150
clustering = kmeans(model,centers=centers,iter.max = 40)

sapply(sample(1:centers,10),function(n) {
  names(clustering$cluster[clustering$cluster==n][1:10])
})
```

```{r}
foo<-model %>% closest_to(~"hôpital" + "crise" + "soignant" + "malades" + "patients",30)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de l'hôpital")
g1
```

```{r solidarité}
foo<-model %>% closest_to(~"solidarité" + "lutte",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de l'hôpital")
g1
```

```{r}
foo<-model %>% closest_to(~"masques" + "distance",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la protection")
g1
```

```{r}
foo<-model %>% closest_to(~"politique" + "gouvernement",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la politique en temps de confinement")
g1
```


```{r le lieu du confinement}
foo<-model %>% closest_to(~"confiné"+"appartement" + "maison" + "studio",40)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches du lieu de confinement")
g1
```

```{r Télétravail, fig.height=7, fig.width=7}
q_words = c("télétravail", "travail", "internet", "connexion")
term_set = lapply(q_words, 
                  function(q_word) {
                    nearest_words = model %>% closest_to(model[[q_word]],25)
                    nearest_words$word
                  }) %>% unlist
subset = model[[term_set,average=F]]

library(Rtsne)
library(RColorBrewer)
# run Rtsne with default parameters
set.seed(57)
rtsne_out <- Rtsne(as.matrix(subset), perplexity=5)
# plot the output of Rtsne into d:\\barneshutplot.jpg file of 2400x1800 dimension
#jpeg("fig.jpg", width=2400, height=1800)
color.vec = c("#556270", "#4ECDC4", "#1B676B", "#FF6B6B", "#C44D58", "seagreen1", "seagreen4", "slateblue4")
  brewer.pal(8, "Set3")
clus<-as.data.frame(clus)
clus$word<-rownames(clus)
terms<-as.data.frame(rownames(subset))
terms$word<-terms[,1] 
terms<-terms %>% left_join(clus, by = "word")

plot(rtsne_out$Y, t='n')
#count(terms, clus)$n[2]
text(rtsne_out$Y, labels=rownames(subset),cex=1.1,col=color.vec[terms$clus])
```