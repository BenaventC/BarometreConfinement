---
title: "Vectorisation"
author: "Valentin Mesa"
date: "5 avril 2020"
output: html_document
---

```{r setup,include=FALSE}
knitr::opts_chunk$set(include=TRUE,echo = TRUE, warning=FALSE,  message=FALSE, cache=TRUE,fig.width = 8)
if (!require(wordVectors)) {
  if (!(require(devtools))) {
    install.packages("devtools")
  }
  devtools::install_github("bmschmidt/wordVectors")
}
library(readr)
library(stringi)
library(tidyverse)
library(gridExtra)
library(cleanNLP) 
library(text2vec)
library(reshape2)
library(wordVectors)
library(tsne)
library(ape)
```

## L'objectif de la vectorisation :

Un outil pour constituer des dictionnaires spécifiques. Pour le corpus "Confinementjour"" les thèmes de prédictions sont les marqueurs de l'expérience du confinement :

 - l'ennui
 - la peur
 - l'ironie
 - agrements et désagrément du teletravail
 - le rapports aux enfants
 - la déréalisation , science-fiction
 - l'approvisionnement et l'alimentation 
 
 l'enjeu : retrouver les termes qui se rapproche le plus du concept que l'on cherche à cerner.
 
 Processus : 
  1) analyse quali des tweet et définition des thèmes
  2) établissement d'un petits nombre de mots cible, utilisation de la synonymie ( Wolf, https://wonef.fr/contact/)
  3) recherche des vecteurs les plus proches et 
  4) réitération en à l'étape 2
 
 
## Processus de préparation de donnée

La condition est faite pour éviter de refaire les calculs qui sont très longs ( en plusieurs heures)

On elimine :

On travaille sur des lemmes.


```{r prep}
if (!file.exists("confinement_vec.txt")) {
  #si le fichier n'existe pas, on lit nos données brutes, et on nettoie le corpus des stop words
  cnlp_init_udpipe(model_name = "french")
  #lecture de l'ensemble de nos tweets
  obj<-readRDS(file = "df_nrcliwc17.rds")
  #suppression des emojis, handlers, hashtags et URLs
  
  cleanText <- function(x) {
       gsub("(@\\S+)*(#\\S+)*(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)*", "", x)
  }
  
  
  obj['text'] = lapply(obj['text'], cleanText)
  #on enlève les tweets qui ne contenaient que des emojis, hashtags ou urls.
  obj = obj[grepl("\\w+",obj$text),]
  #Annotation des tweets afin de pouvoir identifier les stopwords
  Vocab<-cnlp_annotate(obj)
  write_rds(Vocab,"Vocab_17.rds")
  Vocab<-readRDS(file = "Vocab_17.rds")

  #filtrage sur les stopwords
  updated_vocab = filter(Vocab$token, !(upos %in% c('ADP','DET','SCONJ', 'PRON', 'NUM')))
  all_tweets <- paste(updated_vocab['lemma'], sep= " ")
  write.table(all_tweets, file="tweets.txt")
  #Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
  prep_word2vec(origin="tweets",destination="confinement_vec.txt",lowercase=T,bundle_ngrams=3)
}
prep_word2vec(origin="tweets.txt",destination="confinement_vec.txt",lowercase=T,bundle_ngrams=3)

#Création et entraînement du modèle vectoriel
if (!file.exists("confinement_vec_model.bin")) {
  model = train_word2vec("confinement_vec.txt","confinement_vec.bin",vectors=100
                         ,threads=4,window=10,iter=5,negative_samples=0)
} else model = read.vectors("confinement_vec_model.bin")
```

## Clustering des termes sur la base des vecteurs
```{r}
#Un premier clustering
set.seed(10)
centers = 150
clustering = kmeans(model,centers=centers,iter.max = 40) 


sapply(sample(1:centers,10),function(n) {
  names(clustering$cluster[clustering$cluster==n][1:15])
})
```

## Identification des concepts par une approche vectorielle

L'intérêt des vecteurs c'est qu'il peuvent d'additionner ou se soustraire :

roi-homme+femme = reine

pour reprendre cet exemple fameux. 



```{r}

foo<-model %>% closest_to(~"malade"+"patient" +"hopital"+"crise",40) # + "crise" + "patients" + "malades"

foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
```

```{r solidarité}
foo<-model %>% closest_to(~ "lutte"+"solidarit",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la solidarite")
g1
```

```{r}
foo<-model %>% closest_to(~"distance"+"sociale",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la protection")
g1
```

```{r}
foo<-model %>% closest_to(~"politique" + "gouvernement",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la politique en temps de confinement")
g1
```


```{r le lieu du confinement}
foo<-model %>% closest_to(~"confin"+"appartement" - "maison" + "studio",40)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches du lieu de confinement")
g1
```

```{r Télétravail, fig.height=7, fig.width=7}
q_words = c("teletravail", "travail", "internet", "connexion")
term_set = lapply(q_words, 
                  function(q_word) {
                    nearest_words = model %>% closest_to(model[[q_word]],30)
                    nearest_words$word
                  }) %>% unlist
subset = model[[term_set,average=F]]

library(Rtsne)
library(RColorBrewer)
# run Rtsne with default parameters
set.seed(57)
rtsne_out <- Rtsne(as.matrix(subset), perplexity=5)
# plot the output of Rtsne into d:\\barneshutplot.jpg file of 2400x1800 dimension
#jpeg("fig.jpg", width=2400, height=1800)
color.vec = c("#556270", "#4ECDC4", "#1B676B", "#FF6B6B", "#C44D58", "seagreen1", "seagreen4", "slateblue4")
  brewer.pal(8, "Set3")
#clus<-as.data.frame(clus)
#clus$word<-rownames(clus)
terms<-as.data.frame(rownames(subset))
terms$word<-terms[,1] 
#terms<-terms %>% left_join(clus, by = "word")

plot(rtsne_out$Y, t='n')
#count(terms, clus)$n[2]
text(rtsne_out$Y, labels=rownames(subset),cex=0.7)#col=color.vec[terms$clus])
```