---
title: "Vectorisation"
author: "Valentin Mesa"
date: "5 avril 2020"
output: html_document
---

```{r setup,include=FALSE}
knitr::opts_chunk$set(include=TRUE,echo = TRUE, warning=FALSE,  message=FALSE, cache=TRUE,fig.width = 8)
if (!require(wordVectors)) {
  if (!(require(devtools))) {
    install.packages("devtools")
  }
  devtools::install_github("bmschmidt/wordVectors")
}
library(readr)
library(stringi)
library(tidyverse)
library(gridExtra)
library(cleanNLP) 
library(text2vec)
library(reshape2)
library(wordVectors)
library(tsne)
library(ape)
```

## L'objectif de la vectorisation :

Un outil pour constituer des dictionnaires spécifiques. Pour le corpus "Confinementjour"" les thèmes de prédictions sont les marqueurs de l'expérience du confinement :

 - l'ennui
 - la peur
 - l'ironie
 - agrements et désagrément du teletravail
 - le rapports aux enfants
 - la déréalisation , science-fiction
 - l'approvisionnement et l'alimentation 
 
 l'enjeu : retrouver les termes qui se rapprochent le plus du concept que l'on cherche à cerner.
 
 Processus : 
  1) analyse quali des tweet et définition des thèmes
  2) établissement d'un petits nombre de mots cible, utilisation de la synonymie ( Wolf, https://wonef.fr/contact/)
  3) recherche des vecteurs les plus proches et 
  4) réitération en à l'étape 2
 
 
## Processus de préparation de donnée

La condition est faite pour éviter de refaire les calculs qui sont très longs (plusieurs heures)

La stratégie est d'annoter les contenu par les part of speach, et de selectionner ce qui est sémantiquement évident : les noms propres, les verbes, les adjectifs. On utilisera tant qu'on peut les lemmes. On travaille sur des lemmes.

à réintégrer plus tard. (nettoyage par regex, c'est pas un truc sur lequel on est fort)

cleanText <- function(x) {
       gsub("(@\\S+)*(#\\S+)*(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)*", "", x)
  }

  obj['text'] = lapply(obj['text'], cleanText)
  #on enlève les tweets qui ne contenaient que des emojis, hashtags ou urls.
  obj = obj[grepl("\\w+",obj$text),]


```{r prep, warning=FALSE, message=FALSE}
if (!file.exists("confinement_vec.txt")) {
  #si le fichier n'existe pas, on lit nos données brutes, et on nettoie le corpus des stop words
  cnlp_init_udpipe(model_name  = "french")
  #lecture de l'ensemble de nos tweets
  foo<-readRDS(file = "df_nrcliwclsd_cum55.rds")
  obj<-foo$text
  #Annotation des tweets afin de pouvoir identifier les stopwords
  Vocab<-cnlp_annotate(obj)
  
  write_rds(Vocab,"Vocab55.rds")
  
  Vocab<-readRDS(file = "Vocab55.rds")
  
  #filtrage sur les stopwords
  foo<-as.data.frame(Vocab[c("token")])
  ggplot(foo,aes(x=token.upos))+geom_bar()+coord_flip() +theme_minimal()
  ggplot(foo,aes(x=token.relation))+geom_bar()+coord_flip()

  updated_vocab <- foo %>% filter(token.upos %in% c('ADV','ADJ','VERB', 'NOUN'))
  all_tweets <- paste(updated_vocab['token.lemma'], sep= " ")
  write.table(all_tweets, file="tweets.txt")
  #Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
  prep_word2vec(origin="tweets.txt",destination="confinement_vec.txt",lowercase=T,bundle_ngrams=5)
}
```

```{r train, warning=TRUE, message=TRUE}

#Création et entraînement du modèle vectoriel
if (!file.exists("confinement_vec_model.bin")) {
  model = train_word2vec("confinement_vec.txt","confinement_vec.bin",vectors=250
                         ,threads=4,window=5,iter=10,negative_samples=0,force=TRUE, min_count=200)
} 
else model = read.vectors("confinement_vec_model.bin")
```

## Clustering des termes sur la base des vecteurs

```{r, fig.width=11}
#Un premier clustering
set.seed(10)
centers = 15
clustering = kmeans(model,centers=centers,iter.max = 40) 

sapply(sample(1:centers,10),function(n) {
  names(clustering$cluster[clustering$cluster==n][1:20])
})

```

```{r, fig.width=11}

library(ggrepel)

tsne<-tsne(model,k=2,perplexity=4, epoch=100)
words<-as.data.frame(attributes(clustering[[1]]))
cluster<-as.data.frame(clustering[[1]])
cluster$cluster<-as.factor(cluster[,1])
tsne2<-as.data.frame(tsne)
foo<-cbind(words,tsne2,cluster)

wordfreq<-updated_vocab %>% mutate(n=1)%>%group_by(token.lemma)%>% summarise(n=sum(n))
wordfreq$names<-wordfreq$token.lemma
foo<-foo %>% left_join(wordfreq) %>% arrange(-n) 
foo1<- foo %>% filter(!is.na(n) & n>5000)
ggplot(foo1,aes(x=reorder(names,n),y=n))+geom_bar(stat="identity")+coord_flip()

# 4000 signifie 1 pour 1000 mots
foo2<-foo %>%filter(n>1500 & n<12000)

colors<-c("yellowgreen","steelblue4","springgreen3","tan2","salmon","firebrick","darkblue","chartreuse3","aquamarine4","coral3","darkorchid3","deeppink1","gold1","purple","sienna", "purple")
ggplot(foo2,aes(x=V1,y=V2,group=cluster))+theme_minimal()+
  geom_text(aes(label = token.lemma,size=n/10000, color=cluster))+scale_color_manual(values=colors)

```

## Identification des concepts par une approche vectorielle

L'intérêt des vecteurs c'est qu'il peuvent d'additionner ou se soustraire :

roi-homme+femme = reine

pour reprendre cet exemple fameux. 

L'utilisation

```{r}

foo<-model %>% closest_to(~"masque",40)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
```

```{r solidarité}
foo<-model %>% closest_to(~ "masque",60)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la solidarite")
g1
```

```{r}
foo<-model %>% closest_to(~"distance"+"sociale",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la protection")
g1
```

```{r}
foo<-model %>% closest_to(~"politique" + "gouvernement",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la politique en temps de confinement")
g1
```


```{r le lieu du confinement}
foo<-model %>% closest_to(~"confin"+"appartement" - "maison" + "studio",40)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches du lieu de confinement")
g1
```

```{r Télétravail, fig.height=7, fig.width=7}
q_words = c("teletravail", "travail", "internet", "connexion")
term_set = lapply(q_words, 
                  function(q_word) {
                    nearest_words = model %>% closest_to(model[[q_word]],40)
                    nearest_words$word
                  }) %>% unlist
subset = model[[term_set,average=F]]

library(Rtsne)
library(RColorBrewer)
# run Rtsne with default parameters
set.seed(57)
rtsne_out <- Rtsne(as.matrix(subset), perplexity=5)
# plot the output of Rtsne into d:\\barneshutplot.jpg file of 2400x1800 dimension
#jpeg("fig.jpg", width=2400, height=1800)
color.vec = c("#556270", "#4ECDC4", "#1B676B", "#FF6B6B", "#C44D58", "seagreen1", "seagreen4", "slateblue4")
  brewer.pal(8, "Set3")
#clus<-as.data.frame(clus)
#clus$word<-rownames(clus)
terms<-as.data.frame(rownames(subset))
terms$word<-terms[,1] 
#terms<-terms %>% left_join(clus, by = "word")

plot(rtsne_out$Y, t='n')
#count(terms, clus)$n[2]
text(rtsne_out$Y, labels=rownames(subset),cex=0.7)#col=color.vec[terms$clus])
```